<!-- RULEBOOK:START -->
# Project Rules

Generated by @hivellm/rulebook
Generated at: 2025-10-25T20:10:47.555Z

## Documentation Standards

**CRITICAL**: Minimize Markdown files. Keep documentation organized.

### Allowed Root-Level Documentation
Only these files are allowed in the project root:
- ✅ `README.md` - Project overview and quick start
- ✅ `CHANGELOG.md` - Version history and release notes
- ✅ `AGENTS.md` - This file (AI assistant instructions)
- ✅ `LICENSE` - Project license
- ✅ `CONTRIBUTING.md` - Contribution guidelines
- ✅ `CODE_OF_CONDUCT.md` - Code of conduct
- ✅ `SECURITY.md` - Security policy

### All Other Documentation
**ALL other documentation MUST go in `/docs` directory**:
- `/docs/ARCHITECTURE.md` - System architecture
- `/docs/DEVELOPMENT.md` - Development guide
- `/docs/ROADMAP.md` - Project roadmap
- `/docs/DAG.md` - Component dependencies (DAG)
- `/docs/specs/` - Feature specifications
- `/docs/sdks/` - SDK documentation
- `/docs/protocols/` - Protocol specifications
- `/docs/guides/` - Developer guides
- `/docs/diagrams/` - Architecture diagrams
- `/docs/benchmarks/` - Performance benchmarks
- `/docs/versions/` - Version release reports

## Testing Requirements

**CRITICAL**: All features must have comprehensive tests.

- **Minimum Coverage**: 95%
- **Test Location**: `/tests` directory in project root
- **Test Execution**: 100% of tests MUST pass before moving to next task
- **Test First**: Write tests based on specifications before implementation

## Feature Development Workflow

**CRITICAL**: Follow this workflow for all feature development.

1. **Check Specifications First**:
   - Read `/docs/specs/` for feature specifications
   - Review `/docs/ARCHITECTURE.md` for system design
   - Check `/docs/ROADMAP.md` for implementation timeline
   - Review `/docs/DAG.md` for component dependencies

2. **Implement with Tests**:
   - Write tests in `/tests` directory first
   - Implement feature following specifications
   - Ensure tests pass and meet coverage threshold

3. **Quality Checks**:
   - Run code formatter
   - Run linter (must pass with no warnings)
   - Run all tests (must be 100% passing)
   - Verify coverage meets threshold

4. **Update Documentation**:
   - Update `/docs/ROADMAP.md` progress
   - Update feature specs if implementation differs
   - Document any deviations with justification

## Rules Configuration

Rules can be selectively disabled using `.rulesignore` file in project root.

Example `.rulesignore`:
```
# Ignore coverage requirement
coverage-threshold
# Ignore specific language rules
rust/edition-2024
# Ignore all TypeScript rules
typescript/*
```

<!-- RULEBOOK:END -->


<!-- RUST:START -->
# Rust Project Rules

## Rust Edition and Toolchain

**CRITICAL**: Always use Rust Edition 2024 with nightly toolchain.

- **Edition**: 2024
- **Toolchain**: nightly 1.85+
- **Update**: Run `rustup update nightly` regularly

## Code Quality Standards

### Mandatory Quality Checks

**CRITICAL**: After implementing ANY feature, you MUST run these commands in order:

```bash
# 1. Format code (nightly required for Edition 2024)
cargo +nightly fmt --all

# 2. Check for warnings (MUST pass with no warnings)
cargo clippy --workspace -- -D warnings

# 3. Check all targets and features
cargo clippy --workspace --all-targets --all-features -- -D warnings

# 4. Run all tests (MUST pass 100%)
cargo test --workspace --tests --verbose

# 5. Check coverage (MUST meet threshold)
cargo llvm-cov --all --ignore-filename-regex 'examples'
```

**If ANY of these fail, you MUST fix the issues before committing.**

### Formatting

- Use `rustfmt` with nightly toolchain
- Configuration in `rustfmt.toml` or `.rustfmt.toml`
- Always format before committing: `cargo +nightly fmt --all`
- CI must check formatting: `cargo +nightly fmt --all -- --check`

### Linting

- Use `clippy` with `-D warnings` (warnings as errors)
- Fix all clippy warnings before committing
- Acceptable exceptions must be documented with `#[allow(clippy::...)]` and justification
- CI must enforce clippy: `cargo clippy --workspace -- -D warnings`

### Testing

- **Location**: Tests in `/tests` directory for integration tests
- **Unit Tests**: In same file as implementation with `#[cfg(test)]`
- **Coverage**: Must meet project threshold (default 95%)
- **Tools**: Use `cargo-nextest` for faster test execution
- **Async**: Use `tokio::test` for async tests with Tokio runtime

Example test structure:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_feature() {
        // Test implementation
    }

    #[tokio::test]
    async fn test_async_feature() {
        // Async test implementation
    }
}
```

## Async Programming

**CRITICAL**: Follow Tokio best practices for async code.

- **Runtime**: Use Tokio for async runtime
- **Blocking**: Never block in async context - use `spawn_blocking` for CPU-intensive tasks
- **Channels**: Use `tokio::sync::mpsc` or `tokio::sync::broadcast` for async communication
- **Timeouts**: Always set timeouts for network operations: `tokio::time::timeout`

Example:
```rust
use tokio::time::{timeout, Duration};

async fn fetch_data() -> Result<Data, Error> {
    timeout(Duration::from_secs(30), async {
        // Network operation
    }).await?
}
```

## Dependency Management

**CRITICAL**: Always verify latest versions before adding dependencies.

### Before Adding Any Dependency

1. **Check Context7 for latest version**:
   - Use MCP Context7 tool if available
   - Search for the crate documentation
   - Verify the latest stable version
   - Review breaking changes and migration guides

2. **Example Workflow**:
   ```
   Adding tokio → Check crates.io and docs.rs
   Adding serde → Verify latest version with security updates
   Adding axum → Check for breaking changes in latest version
   ```

3. **Document Version Choice**:
   - Note why specific version chosen in `Cargo.toml` comments
   - Document any compatibility constraints
   - Update CHANGELOG.md with new dependencies

### Dependency Guidelines

- ✅ Use latest stable versions
- ✅ Check for security advisories: `cargo audit`
- ✅ Prefer well-maintained crates (active development, good documentation)
- ✅ Minimize dependency count
- ✅ Use workspace dependencies for monorepos
- ❌ Don't use outdated versions without justification
- ❌ Don't add dependencies without checking latest version

## Codespell Configuration

**CRITICAL**: Use codespell to catch typos in code and documentation.

Install: `pip install 'codespell[toml]'`

Configuration in `pyproject.toml`:
```toml
[tool.codespell]
skip = "*.lock,*.json,target,node_modules,.git"
ignore-words-list = "crate,ser,deser"
```

Or run with flags:
```bash
codespell \
  --skip="*.lock,*.json,target,node_modules,.git" \
  --ignore-words-list="crate,ser,deser"
```

## Error Handling

- Use `Result<T, E>` for recoverable errors
- Use `thiserror` for custom error types
- Use `anyhow` for application-level error handling
- Document error conditions in function docs
- Never use `unwrap()` or `expect()` in production code without justification

Example:
```rust
use thiserror::Error;

#[derive(Error, Debug)]
pub enum MyError {
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    
    #[error("Invalid input: {0}")]
    InvalidInput(String),
}

pub fn process_data(input: &str) -> Result<Data, MyError> {
    // Implementation
}
```

## Documentation

- **Public APIs**: Must have doc comments (`///`)
- **Examples**: Include examples in doc comments
- **Modules**: Document module purpose with `//!`
- **Unsafe**: Always document safety requirements for `unsafe` code
- **Run doctests**: `cargo test --doc`

Example:
```rust
/// Processes the input data and returns a result.
///
/// # Arguments
///
/// * `input` - The input string to process
///
/// # Examples
///
/// ```
/// use mylib::process;
/// let result = process("hello");
/// assert_eq!(result, "HELLO");
/// ```
///
/// # Errors
///
/// Returns `MyError::InvalidInput` if input is empty.
pub fn process(input: &str) -> Result<String, MyError> {
    // Implementation
}
```

## Project Structure

```
project/
├── Cargo.toml          # Package manifest
├── Cargo.lock          # Dependency lock file (commit this)
├── README.md           # Project overview (allowed in root)
├── CHANGELOG.md        # Version history (allowed in root)
├── AGENTS.md          # AI assistant rules (allowed in root)
├── LICENSE            # Project license (allowed in root)
├── CONTRIBUTING.md    # Contribution guidelines (allowed in root)
├── CODE_OF_CONDUCT.md # Code of conduct (allowed in root)
├── SECURITY.md        # Security policy (allowed in root)
├── src/
│   ├── lib.rs          # Library root (for libraries)
│   ├── main.rs         # Binary root (for applications)
│   └── ...
├── tests/              # Integration tests
├── examples/           # Example code
├── benchmark/          # Benchmarks
└── docs/               # Project documentation
```

## CI/CD Requirements

Must include GitHub Actions workflows for:

1. **Testing** (`rust-test.yml`):
   - Test on ubuntu-latest, windows-latest, macos-latest
   - Use `cargo-nextest` for fast test execution
   - Upload test results

2. **Linting** (`rust-lint.yml`):
   - Format check: `cargo +nightly fmt --all -- --check`
   - Clippy: `cargo clippy --workspace -- -D warnings`
   - All targets: `cargo clippy --workspace --all-targets -- -D warnings`

3. **Codespell** (`codespell.yml`):
   - Check for typos in code and documentation
   - Fail on errors

## Crate Publication

### Publishing to crates.io

**Prerequisites:**
1. Create account at https://crates.io
2. Generate API token: `cargo login`
3. Add `CARGO_TOKEN` to GitHub repository secrets

**Cargo.toml Configuration:**

```toml
[package]
name = "your-crate-name"
version = "1.0.0"
edition = "2024"
authors = ["Your Name <your.email@example.com>"]
license = "MIT OR Apache-2.0"
description = "A short description of your crate"
documentation = "https://docs.rs/your-crate-name"
homepage = "https://github.com/your-org/your-crate-name"
repository = "https://github.com/your-org/your-crate-name"
readme = "README.md"
keywords = ["your", "keywords", "here"]
categories = ["category"]
exclude = [
    ".github/",
    "tests/",
    "benchmark/",
    "examples/",
    "*.sh",
]

[package.metadata.docs.rs]
all-features = true
rustdoc-args = ["--cfg", "docsrs"]
```

**Publishing Workflow:**

1. Update version in Cargo.toml
2. Update CHANGELOG.md
3. Run quality checks:
   ```bash
   cargo fmt --all
   cargo clippy --workspace --all-targets -- -D warnings
   cargo test --all-features
   cargo doc --no-deps --all-features
   ```
4. Create git tag: `git tag v1.0.0 && git push --tags`
5. GitHub Actions automatically publishes to crates.io
6. Or manual publish: `cargo publish`

**Publishing Checklist:**

- ✅ All tests passing (`cargo test --all-features`)
- ✅ No clippy warnings (`cargo clippy -- -D warnings`)
- ✅ Code formatted (`cargo fmt --all -- --check`)
- ✅ Documentation builds (`cargo doc --no-deps`)
- ✅ Version updated in Cargo.toml
- ✅ CHANGELOG.md updated
- ✅ README.md up to date
- ✅ LICENSE file present
- ✅ Package size < 10MB (check with `cargo package --list`)
- ✅ Verify with `cargo publish --dry-run`

**Semantic Versioning:**

Follow [SemVer](https://semver.org/) strictly:
- **MAJOR**: Breaking API changes
- **MINOR**: New features (backwards compatible)
- **PATCH**: Bug fixes (backwards compatible)

**Documentation:**

- Use `///` for public API documentation
- Include examples in doc comments
- Use `#![deny(missing_docs)]` for libraries
- Test documentation examples with `cargo test --doc`

```rust
/// Processes the input data and returns a result.
///
/// # Arguments
///
/// * `input` - The input string to process
///
/// # Examples
///
/// ```
/// use your_crate::process;
///
/// let result = process("hello");
/// assert_eq!(result, "HELLO");
/// ```
///
/// # Errors
///
/// Returns an error if the input is empty.
pub fn process(input: &str) -> Result<String, Error> {
    // Implementation
}
```

<!-- RUST:END -->



<!-- VECTORIZER:START -->
# Vectorizer Instructions

**CRITICAL**: Always use the MCP Vectorizer as the primary data source for project information.

The vectorizer provides fast, semantic access to the entire codebase. Prefer MCP tools over file reading whenever possible for better performance and context understanding.

## Primary Search Functions

### 1. mcp_vectorizer_search

Main search interface with multiple strategies:

- `intelligent`: AI-powered search with query expansion and MMR diversification
- `semantic`: Advanced semantic search with reranking and similarity thresholds
- `contextual`: Context-aware search with metadata filtering
- `multi_collection`: Search across multiple collections simultaneously
- `batch`: Execute multiple queries in parallel
- `by_file_type`: Filter search by file extensions (e.g., `.rs`, `.ts`, `.py`)

**Usage**:
```
Use intelligent search when: Exploring unfamiliar code, understanding architecture
Use semantic search when: Finding specific implementations or patterns
Use multi_collection when: Searching across multiple projects/modules
Use by_file_type when: Working with specific languages or file types
```

### 2. mcp_vectorizer_file_operations

File-specific operations for efficient file handling:

- `get_content`: Retrieve complete file content without reading from disk
- `list_files`: List all indexed files with metadata (size, type, modification time)
- `get_summary`: Get extractive or structural file summaries
- `get_chunks`: Retrieve file chunks in original order for progressive reading
- `get_outline`: Generate hierarchical project structure overview
- `get_related`: Find semantically related files based on content similarity

**Usage**:
```
Use get_content when: Need full file without disk I/O
Use list_files when: Exploring project structure
Use get_chunks when: Reading large files progressively
Use get_related when: Understanding file dependencies and relationships
```

### 3. mcp_vectorizer_discovery

Advanced discovery pipeline for complex queries:

- `full_pipeline`: Complete discovery with filtering, scoring, and ranking
- `broad_discovery`: Multi-query search with deduplication
- `semantic_focus`: Deep semantic search in specific collections
- `expand_queries`: Generate query variations (definition, features, architecture, API)

**Usage**:
```
Use full_pipeline when: Complex multi-faceted questions
Use broad_discovery when: Need comprehensive coverage of a topic
Use expand_queries when: Uncertain about exact terminology
```

## Best Practices

1. **Start with intelligent search** for exploratory queries to understand codebase structure
2. **Use file_operations** when you need complete file context without disk access
3. **Use discovery pipeline** for complex, multi-faceted questions requiring deep analysis
4. **Prefer batch operations** when searching for multiple related items to reduce latency
5. **Use by_file_type** when working with specific languages (e.g., only Rust or TypeScript files)

## Performance Tips

- **Batch queries** instead of sequential searches for better performance
- **Use specific collections** when you know the target area to reduce search space
- **Set similarity thresholds** to filter out irrelevant results (typically 0.6-0.8)
- **Cache results** for repeated queries within the same session

## Common Patterns

### Pattern 1: Understanding a Feature
```
1. Use intelligent search to find feature implementation
2. Use get_related to find connected files
3. Use get_outline to understand feature structure
4. Use get_content to read specific implementations
```

### Pattern 2: Debugging an Issue
```
1. Use semantic search with error message or symptom
2. Use by_file_type to focus on relevant language files
3. Use get_chunks to progressively read large files
4. Use get_related to find potentially affected files
```

### Pattern 3: Adding a New Feature
```
1. Use expand_queries to find similar existing features
2. Use full_pipeline for comprehensive discovery
3. Use get_outline to understand where to add code
4. Use get_related to find integration points
```

<!-- VECTORIZER:END -->



<!-- SYNAP:START -->
# Synap Instructions

**CRITICAL**: Use MCP Synap for persistent task and data storage during development sessions.

Synap provides a distributed key-value store, pub/sub messaging, and streaming capabilities. Use it to maintain state, track tasks, and persist important data across context windows.

## Core Features

### 1. Key-Value Store

Store and retrieve data with TTL support:

- `synap_kv_get`: Retrieve a value by key
- `synap_kv_set`: Store a value with optional TTL (time-to-live)
- `synap_kv_delete`: Remove a key from storage
- `synap_kv_scan`: Scan keys by prefix pattern

**Usage**:
```
Use for: Task tracking, configuration storage, session state
TTL for: Temporary data that should expire
Scan for: Listing related items by prefix (e.g., "task:*")
```

### 2. Queue System

Persistent message queues for task management:

- `synap_queue_publish`: Add a message to a queue with priority
- `synap_queue_consume`: Retrieve and process messages from queue

**Usage**:
```
Use for: Task queues, work distribution, async job processing
Priorities: 0-9 (9 = highest priority)
Pattern: Producer-consumer model for parallel work
```

### 3. Pub/Sub Messaging

Event-driven communication:

- `synap_pubsub_publish`: Broadcast message to topic subscribers

**Usage**:
```
Use for: Event notifications, real-time updates, broadcast messages
Pattern: One-to-many messaging for loosely coupled components
```

### 4. Streaming

Real-time event streaming:

- `synap_stream_publish`: Publish events to a stream room

**Usage**:
```
Use for: Real-time data streams, live updates, event logs
Pattern: Continuous data flow for monitoring and analytics
```

## Best Practices for AI Development

### Task Tracking

Store implementation tasks and progress:

```
Pattern: "task:<feature-name>:<subtask-id>"

Example:
- synap_kv_set("task:auth:implement-login", JSON.stringify({
    status: "in_progress",
    started: "2024-01-01T10:00:00Z",
    tests: ["test_login_success", "test_login_failure"],
    coverage: 95.2
  }))
```

### Session State

Preserve state across context windows:

```
Pattern: "session:<session-id>:<data-type>"

Example:
- synap_kv_set("session:abc123:current-file", "/src/auth/login.ts")
- synap_kv_set("session:abc123:todo-list", JSON.stringify([...]))
```

### Configuration Storage

Store project configuration and settings:

```
Pattern: "config:<category>:<key>"

Example:
- synap_kv_set("config:project:coverage-threshold", "95")
- synap_kv_set("config:project:languages", JSON.stringify(["rust", "typescript"]))
```

### Test Results

Track test execution and coverage:

```
Pattern: "test:<suite>:<timestamp>"

Example:
- synap_kv_set("test:integration:latest", JSON.stringify({
    passed: 42,
    failed: 0,
    coverage: 96.5,
    duration: "3.2s"
  }), 86400) // TTL: 24 hours
```

## Common Patterns

### Pattern 1: Multi-Step Implementation Tracking

```
1. Store overall plan: synap_kv_set("plan:feature-x", plan_json)
2. Track each step: synap_kv_set("step:feature-x:1", step_status)
3. Update progress: synap_kv_set("progress:feature-x", percentage)
4. Mark complete: synap_kv_delete("plan:feature-x")
```

### Pattern 2: Code Generation History

```
1. Store generated code: synap_kv_set("generated:file-path", code)
2. Track modifications: synap_kv_set("history:file-path", changelog)
3. List all generated: synap_kv_scan("generated:*")
```

### Pattern 3: Error Tracking

```
1. Log errors: synap_kv_set("error:timestamp", error_details, 3600)
2. Track fixes: synap_kv_set("fix:error-id", fix_details)
3. Scan recent errors: synap_kv_scan("error:*")
```

## Retention and Cleanup

- Use TTL for temporary data (session state, cache, recent errors)
- No TTL for persistent data (configuration, important results)
- Regularly clean up old data with `synap_kv_delete`
- Use prefixes for easy bulk operations with `synap_kv_scan`

## Integration with Development Workflow

1. **Before Starting**: Check for existing state (`synap_kv_get("session:current-task")`)
2. **During Work**: Update progress regularly (`synap_kv_set("progress:*")`)
3. **After Completion**: Store results and clean up temporary data
4. **Context Switch**: Save complete state before summarization

<!-- SYNAP:END -->



<!-- OPENSPEC:START -->
# OpenSpec Instructions

**CRITICAL**: Use OpenSpec for managing change proposals, specifications, and feature planning.

OpenSpec provides a structured workflow for proposing, reviewing, and implementing changes to the project. Always check for existing specs before implementing features.

## When to Use OpenSpec

Use OpenSpec when the request:

- Mentions planning or proposals (words like "proposal", "spec", "change", "plan")
- Introduces new capabilities or features
- Involves breaking changes or API modifications
- Requires architecture shifts or major refactoring
- Includes significant performance or security work
- Sounds ambiguous and needs clarification before coding

## OpenSpec Workflow

### 1. Check Existing Specs

Before implementing any feature:

```
1. Check /openspec directory for existing proposals
2. Review @/openspec/AGENTS.md for conventions
3. Search for related specs or RFCs
4. Read relevant specifications thoroughly
```

### 2. Create New Proposal

For new features or significant changes:

```
1. Use OpenSpec format for proposal
2. Include rationale, design, alternatives
3. Document breaking changes
4. Specify acceptance criteria
5. Request review from maintainers
```

### 3. Implementation

Follow the approved spec:

```
1. Implement exactly as specified
2. Document any deviations with justification
3. Update spec if design changes during implementation
4. Reference spec in commit messages
5. Mark spec as implemented when complete
```

### 4. Review and Iterate

Continuous improvement:

```
1. Collect feedback during implementation
2. Update spec with learnings
3. Document issues and resolutions
4. Archive completed specs appropriately
```

## Spec Format

Follow the OpenSpec conventions:

```markdown
# [Spec Title]

## Status
[Draft | Under Review | Approved | Implemented | Rejected]

## Summary
Brief description of the proposal.

## Motivation
Why is this change needed?

## Detailed Design
How will this be implemented?

## Alternatives Considered
What other approaches were considered?

## Breaking Changes
Any breaking changes and migration path.

## Acceptance Criteria
How to verify the implementation is complete.
```

## Best Practices

1. **Read Before Implementing**: Always check `/openspec/AGENTS.md` first
2. **Follow Conventions**: Use the specified format and structure
3. **Be Thorough**: Include all relevant details and considerations
4. **Update Actively**: Keep specs current during implementation
5. **Reference Specs**: Link specs in commits, PRs, and documentation

## Integration with Development Workflow

### Before Starting Work

```
1. Check: Does a spec exist for this feature?
2. If yes: Read and understand the complete specification
3. If no: Should this have a spec? (significant features should)
4. If needed: Create proposal and get approval before implementing
```

### During Implementation

```
1. Follow the spec exactly
2. Document deviations immediately
3. Update spec if design changes
4. Add implementation notes to spec
```

### After Completion

```
1. Mark spec as implemented
2. Update with actual implementation details
3. Document any differences from original design
4. Archive or move to appropriate location
```

## Common Patterns

### Pattern 1: New Feature

```
1. Create proposal in /openspec/proposals/
2. Get feedback and approval
3. Implement according to spec
4. Update /docs/specs/ with final design
5. Mark proposal as implemented
```

### Pattern 2: Breaking Change

```
1. Create RFC (Request for Comments)
2. Document migration path
3. Get community/team review
4. Implement with migration guide
5. Update CHANGELOG.md and version docs
```

### Pattern 3: Architecture Change

```
1. Create architecture proposal
2. Include diagrams and detailed design
3. Review with architecture team
4. Implement in phases if needed
5. Update /docs/ARCHITECTURE.md
```

## File Organization

```
/openspec/
├── AGENTS.md          # OpenSpec conventions
├── proposals/         # Active proposals
├── rfcs/              # Requests for comments
├── implemented/       # Completed specs
└── rejected/          # Declined proposals
```

<!-- OPENSPEC:END -->



<!-- CONTEXT7:START -->
# Context7 Instructions

**CRITICAL**: Use MCP Context7 to access up-to-date library documentation before adding dependencies.

Context7 provides real-time access to documentation for thousands of libraries and frameworks. Always check Context7 before adding new dependencies to ensure you're using the latest stable versions and following best practices.

## Core Functions

### 1. resolve-library-id

Resolve a package name to a Context7-compatible library ID:

```
Input: Library name (e.g., "tokio", "react", "fastapi")
Output: Context7 library ID (e.g., "/tokio-rs/tokio", "/facebook/react")
```

**MUST** use this function before `get-library-docs` unless the user provides an explicit library ID.

### 2. get-library-docs

Fetch documentation for a library:

```
Input: Context7 library ID, optional topic, token limit
Output: Relevant documentation, examples, API reference
```

Options:
- `topic`: Focus on specific area (e.g., "routing", "hooks", "async")
- `tokens`: Control documentation size (default: 5000)

## Mandatory Usage

### Before Adding Dependencies

**CRITICAL**: Check Context7 for every new dependency.

#### Rust Example
```
Adding tokio:
1. resolve-library-id("tokio") → "/tokio-rs/tokio"
2. get-library-docs("/tokio-rs/tokio") → Latest version, features, examples
3. Check for breaking changes in latest version
4. Add to Cargo.toml with correct version and features
```

#### TypeScript Example
```
Adding express:
1. resolve-library-id("express") → "/expressjs/express"
2. get-library-docs("/expressjs/express") → Latest version, middleware patterns
3. Review TypeScript type definitions availability
4. Add to package.json with latest stable version
```

#### Python Example
```
Adding fastapi:
1. resolve-library-id("fastapi") → "/tiangolo/fastapi"
2. get-library-docs("/tiangolo/fastapi", topic="async") → Async patterns
3. Check Python version requirements
4. Add to pyproject.toml or requirements.txt
```

## Best Practices

### 1. Version Verification

Always verify the latest stable version:

```
1. Use resolve-library-id to find the library
2. Use get-library-docs to see current version
3. Check for security advisories
4. Review changelog for breaking changes
5. Document version choice in code/commits
```

### 2. Topic-Focused Queries

Use the topic parameter for specific information:

```
Examples:
- get-library-docs("/tokio-rs/tokio", topic="channels")
- get-library-docs("/facebook/react", topic="hooks")
- get-library-docs("/psf/requests", topic="authentication")
```

### 3. Migration Guides

When updating major versions:

```
1. Get docs for current version
2. Get docs for target version
3. Look for migration guide in documentation
4. Review breaking changes
5. Plan migration strategy
```

### 4. Best Practices Discovery

Learn idiomatic usage patterns:

```
1. Get library docs with relevant topic
2. Review code examples
3. Check for recommended patterns
4. Follow security best practices
5. Implement according to documentation
```

## Integration with Development Workflow

### Adding New Dependency

```
1. Identify need for library
2. Use resolve-library-id to find correct library
3. Use get-library-docs to review:
   - Latest stable version
   - Features and capabilities
   - Usage examples
   - Security considerations
4. Add dependency with correct version
5. Document why this library was chosen
6. Update CHANGELOG.md
```

### Updating Existing Dependency

```
1. Use get-library-docs for current version
2. Use get-library-docs for latest version
3. Review changelog between versions
4. Check for breaking changes
5. Update code if needed
6. Update dependency version
7. Test thoroughly
```

### Troubleshooting

```
1. Use get-library-docs with specific topic
2. Search for error message or issue
3. Review examples for correct usage
4. Check for known issues or workarounds
5. Verify you're following best practices
```

## Common Patterns

### Pattern 1: Dependency Selection

```
Problem: Need HTTP client library

1. resolve-library-id("requests") for Python
   OR resolve-library-id("reqwest") for Rust
   OR resolve-library-id("axios") for TypeScript

2. get-library-docs for each candidate

3. Compare features, performance, maintenance

4. Choose best fit for requirements

5. Document decision
```

### Pattern 2: Feature Discovery

```
Need: Async file operations in Rust

1. resolve-library-id("tokio") → "/tokio-rs/tokio"
2. get-library-docs("/tokio-rs/tokio", topic="file I/O")
3. Review async file operation examples
4. Implement using documented patterns
```

### Pattern 3: Security Verification

```
Before adding crypto library:

1. resolve-library-id("ring") for Rust
2. get-library-docs("/briansmith/ring")
3. Check security audit status
4. Review recommended algorithms
5. Verify actively maintained
6. Add with appropriate features
```

## Library ID Format

Context7 library IDs follow patterns:

- GitHub: `/org/repo` or `/org/repo/version`
- Examples:
  - `/tokio-rs/tokio`
  - `/vercel/next.js`
  - `/psf/requests`
  - `/vercel/next.js/v14.0.0` (specific version)

## Error Handling

If library not found:

1. Verify correct library name
2. Try alternative names or repos
3. Check if library is on supported platforms
4. Consider using official documentation as fallback

<!-- CONTEXT7:END -->



<!-- GIT:START -->

**AI Assistant Git Push Mode**: MANUAL

**CRITICAL**: Never execute `git push` commands automatically.
Always provide push commands for manual execution by the user.

Example:
```
✋ MANUAL ACTION REQUIRED:
Run these commands manually (SSH password may be required):
  git push origin main
  git push origin v1.0.0
```

# Git Workflow Rules

**CRITICAL**: Specific rules and patterns for Git version control workflow.

## Git Workflow Overview

This project follows a strict Git workflow to ensure code quality and proper version control.

**NEVER commit code without tests passing. NEVER create tags without full quality checks.**

## Initial Repository Setup

### New Project Initialization

**⚠️ CRITICAL**: Only run initialization commands if `.git` directory does NOT exist!

```bash
# Check if Git repository already exists
if [ -d .git ]; then
  echo "❌ Git repository already initialized. Skipping git init."
  echo "Current status:"
  git status
  git remote -v
  exit 0
fi

# If no .git directory exists, initialize:

# Initialize Git repository
git init

# Add all files
git add .

# Initial commit
git commit -m "chore: Initial project setup"

# Rename default branch to main (GitHub standard)
git branch -M main

# Add remote (if applicable)
git remote add origin <repository-url>
```

**AI Assistant Behavior:**

```
BEFORE running any Git initialization commands:

1. Check if .git directory exists
2. If exists:
   ✅ Repository already configured
   ❌ DO NOT run: git init
   ❌ DO NOT run: git branch -M main
   ✅ Check status: git status
   ✅ Show remotes: git remote -v
   
3. If not exists:
   ✅ Safe to initialize
   ✅ Run full initialization sequence
```

## AI Assistant Git Checks

**CRITICAL**: AI assistants MUST perform these checks before Git operations:

### Automatic Checks

```bash
# 1. Check if Git repository exists
if [ ! -d .git ]; then
  echo "No Git repository found."
  # Ask user if they want to initialize
fi

# 2. Check if there are unstaged changes
git status --short

# 3. Check current branch
CURRENT_BRANCH=$(git branch --show-current)
echo "On branch: $CURRENT_BRANCH"

# 4. Check if remote exists
git remote -v

# 5. Check for unpushed commits
git log origin/main..HEAD --oneline 2>/dev/null
```

### Before Git Commands

**NEVER execute if `.git` directory exists:**
- ❌ `git init` - Repository already initialized
- ❌ `git branch -M main` - Branch may already be configured
- ❌ `git remote add origin` - Remote may already exist (check first with `git remote -v`)
- ❌ `git config user.name/email` - User configuration is personal
- ❌ Reconfiguration commands - Repository is already set up

**ALWAYS safe to execute:**
- ✅ `git status` - Check repository state
- ✅ `git add` - Stage changes
- ✅ `git commit` - Create commits (after quality checks)
- ✅ `git log` - View history
- ✅ `git diff` - View changes
- ✅ `git branch` - List branches
- ✅ `git tag` - Create tags (after quality checks)

**Execute with caution (check first):**
- ⚠️ `git push` - Follow push mode configuration
- ⚠️ `git pull` - May cause merge conflicts
- ⚠️ `git merge` - May cause conflicts
- ⚠️ `git rebase` - Can rewrite history
- ⚠️ `git reset --hard` - Destructive, only for rollback
- ⚠️ `git push --force` - NEVER on main/master

### Repository Detection

**AI Assistant MUST check:**

```bash
# Before ANY Git operation:

# 1. Does .git exist?
if [ -d .git ]; then
  echo "✅ Git repository exists"
  
  # 2. Check current state
  git status
  
  # 3. Check branch
  BRANCH=$(git branch --show-current)
  echo "On branch: $BRANCH"
  
  # 4. Check remote
  REMOTE=$(git remote -v)
  if [ -z "$REMOTE" ]; then
    echo "⚠️  No remote configured"
  else
    echo "Remote: $REMOTE"
  fi
  
  # 5. Proceed with normal Git operations
else
  echo "⚠️  No Git repository found"
  echo "Ask user if they want to initialize Git"
fi
```

## Daily Development Workflow

### 1. Before Making Changes

**CRITICAL**: Always check current state:

```bash
# Check current branch and status
git status

# Ensure you're on the correct branch
git branch

# Pull latest changes if working with team
git pull origin main
```

### 2. Making Changes

**CRITICAL**: Commit after every important implementation:

```bash
# After implementing a feature/fix:

# 1. Run ALL quality checks FIRST
npm run lint           # or equivalent for your language
npm run type-check     # TypeScript/typed languages
npm test              # ALL tests must pass
npm run build         # Ensure build succeeds

# 2. If ALL checks pass, stage changes
git add .

# 3. Commit with conventional commit message
git commit -m "feat: Add user authentication

- Implement login/logout functionality
- Add JWT token management
- Include comprehensive tests (95%+ coverage)
- Update documentation"

# Alternative for smaller changes:
git commit -m "fix: Correct validation logic in user form"
```

### 3. Pushing Changes

**⚠️ IMPORTANT**: Pushing is OPTIONAL and depends on your setup.

```bash
# IF you have passwordless SSH or want to push:
git push origin main

# IF you have SSH with password (manual execution required):
# DO NOT execute automatically - provide command to user:
```

**For users with SSH password authentication:**
```
✋ MANUAL ACTION REQUIRED:

Run this command manually (requires SSH password):
git push origin main
```

**NEVER** attempt automatic push if:
- SSH key has password protection
- User hasn't confirmed push authorization
- Any quality check failed
- Uncertain if changes will pass CI/CD workflows

## Conventional Commits

**MUST** follow conventional commit format:

```bash
# Format: <type>(<scope>): <subject>
#
# <body>
#
# <footer>

# Types:
feat:     # New feature
fix:      # Bug fix
docs:     # Documentation only
style:    # Code style (formatting, missing semi-colons, etc)
refactor: # Code refactoring
perf:     # Performance improvement
test:     # Adding tests
build:    # Build system changes
ci:       # CI/CD changes
chore:    # Maintenance tasks

# Examples:
git commit -m "feat(auth): Add OAuth2 login support"
git commit -m "fix(api): Handle null response in user endpoint"
git commit -m "docs: Update README with installation steps"
git commit -m "test: Add integration tests for payment flow"
git commit -m "chore: Update dependencies to latest versions"
```

## Version Management

### Creating New Version

**CRITICAL**: Full quality gate required before versioning!

```bash
# 1. MANDATORY: Run complete quality suite
npm run lint          # Must pass with no warnings
npm test             # Must pass 100%
npm run type-check   # Must pass (if applicable)
npm run build        # Must succeed
npx codespell        # Must pass (if configured)

# 2. Update version in package.json/Cargo.toml/etc
# Use semantic versioning:
# - MAJOR: Breaking changes (1.0.0 -> 2.0.0)
# - MINOR: New features, backwards compatible (1.0.0 -> 1.1.0)
# - PATCH: Bug fixes (1.0.0 -> 1.0.1)

# 3. Update CHANGELOG.md
# Document all changes in this version:
## [1.2.0] - 2024-01-15
### Added
- New feature X
- New feature Y

### Fixed
- Bug in component Z

### Changed
- Refactored module A

# 4. Commit version changes
git add .
git commit -m "chore: Release version 1.2.0

- Updated version to 1.2.0
- Updated CHANGELOG.md with release notes"

# 5. Create annotated tag
git tag -a v1.2.0 -m "Release version 1.2.0

Major changes:
- Feature X
- Feature Y
- Bug fix Z

All tests passing ✅
Coverage: 95%+ ✅
Linting: Clean ✅
Build: Success ✅"

# 6. OPTIONAL: Push tag (manual if SSH password)
# Only if you're CERTAIN it will pass CI/CD workflows!
```

**For users requiring manual push:**
```
✋ MANUAL ACTIONS REQUIRED:

1. Verify all quality checks passed locally
2. Push commits:
   git push origin main

3. Push tag:
   git push origin v1.2.0

Note: Tag push will trigger CI/CD workflows and may create GitHub release.
Only push if you're confident all checks will pass.
```

## Quality Gate Enforcement

### Before ANY Commit

**MANDATORY CHECKS**:

```bash
# Checklist - ALL must pass:
☐ Code formatted
☐ Linter passes (no warnings)
☐ Type check passes
☐ ALL tests pass (100%)
☐ Coverage meets threshold (95%+)
☐ Build succeeds
☐ No console errors/warnings

# Run quality check script:
npm run quality-check  # or equivalent

# If ANY check fails:
# ❌ DO NOT COMMIT
# ❌ FIX THE ISSUES FIRST
```

### Before Tag Creation

**MANDATORY CHECKS** (even stricter):

```bash
# Extended checklist - ALL must pass:
☐ All pre-commit checks passed
☐ Codespell passes (no typos)
☐ Security audit clean
☐ Dependencies up to date
☐ Documentation updated
☐ CHANGELOG.md updated
☐ Version bumped correctly
☐ All workflows would pass

# Run comprehensive check:
npm run lint
npm test
npm run type-check
npm run build
npx codespell
npm audit

# Only create tag if everything is green!
```

## Error Recovery & Rollback

### When Implementation Is Failing

If the AI is making repeated mistakes and user is frustrated:

```bash
# 1. Identify last stable commit
git log --oneline -10

# 2. Create backup branch of current work
git branch backup-failed-attempt

# 3. Hard reset to last stable version
git reset --hard <last-stable-commit-hash>

# 4. Verify stability
npm test
npm run build

# 5. Reimplement from scratch using DIFFERENT approach
# ⚠️ DO NOT repeat the same techniques that failed before
# ⚠️ Review AGENTS.md for alternative patterns
# ⚠️ Consider different architecture/design

# 6. After successful reimplementation
git branch -D backup-failed-attempt  # Delete backup if no longer needed
```

### Undo Last Commit (Not Pushed)

```bash
# Keep changes, undo commit
git reset --soft HEAD~1

# Discard changes completely
git reset --hard HEAD~1
```

### Revert Pushed Commit

```bash
# Create revert commit
git revert <commit-hash>

# Then push (manual if SSH password)
```

## Branch Strategy

### Feature Branches

```bash
# Create feature branch
git checkout -b feature/user-authentication

# Work on feature...
# Commit regularly with quality checks

# When feature complete and tested:
git checkout main
git merge feature/user-authentication

# Delete feature branch
git branch -d feature/user-authentication
```

### Hotfix Workflow

```bash
# Critical bug in production
git checkout -b hotfix/critical-security-fix

# Fix the bug
# MUST include tests
# MUST pass all quality checks

git commit -m "fix: Critical security vulnerability in auth

- Patch authentication bypass
- Add regression tests
- Update security documentation"

# Merge to main
git checkout main
git merge hotfix/critical-security-fix

# Tag immediately if production fix
git tag -a v1.2.1 -m "Hotfix: Security patch"

# Manual push if required
```

## Critical AI Assistant Rules

### Repository Initialization

**BEFORE any `git init` or setup commands:**

```
1. Check for .git directory existence
2. If .git exists:
   - ❌ STOP - Repository already configured
   - ❌ DO NOT run git init
   - ❌ DO NOT run git config
   - ❌ DO NOT run git branch -M
   - ❌ DO NOT reconfigure anything
   - ✅ Use existing repository as-is
   
3. If .git does NOT exist:
   - ✅ Ask user if they want Git initialization
   - ✅ Run initialization sequence if approved
```

### Push Command Behavior

**Based on configured push mode:**

```
Manual Mode (DEFAULT):
  ❌ NEVER execute: git push
  ✅ ALWAYS provide: "Run manually: git push origin main"
  
Prompt Mode:
  ⚠️  ALWAYS ask first: "Ready to push. Proceed? [Y/n]"
  ✅ Execute only if user confirms
  
Auto Mode:
  ⚠️  Check quality first
  ⚠️  Only if 100% confident
  ✅ Execute if all checks passed
```

### Quality Gate Enforcement

**MANDATORY checks before commit:**

```bash
# Run in this exact order:
1. npm run lint          # or language equivalent
2. npm run type-check    # if applicable
3. npm test             # ALL tests must pass
4. npm run build        # must succeed

# If ANY fails:
❌ STOP - DO NOT commit
❌ Fix issues first
❌ Re-run all checks

# If ALL pass:
✅ Safe to commit
✅ Proceed with git add and commit
```

**MANDATORY checks before tag:**

```bash
# Extended checks for version tags:
1. All commit checks above +
2. npx codespell        # no typos
3. npm audit            # no vulnerabilities
4. CHANGELOG.md updated
5. Version bumped correctly
6. Documentation current

# If ANY fails:
❌ STOP - DO NOT create tag
❌ Fix issues
❌ Re-verify everything

# Only create tag if 100% green!
```

## Best Practices

### DO's ✅

- **ALWAYS** check if .git exists before init commands
- **ALWAYS** run tests before commit
- **ALWAYS** use conventional commit messages
- **ALWAYS** update CHANGELOG for versions
- **COMMIT** after each important implementation
- **TAG** releases with semantic versions
- **VERIFY** quality gates before tagging
- **DOCUMENT** breaking changes clearly
- **REVERT** when implementation is failing repeatedly
- **ASK** user before automatic push
- **PROVIDE** manual commands for SSH password users
- **CHECK** repository state before operations
- **RESPECT** existing Git configuration

### DON'Ts ❌

- **NEVER** run `git init` if .git exists
- **NEVER** run `git config` (user-specific)
- **NEVER** reconfigure existing repository
- **NEVER** commit without passing tests
- **NEVER** commit with linting errors
- **NEVER** commit with build failures
- **NEVER** create tag without quality checks
- **NEVER** push automatically with SSH password
- **NEVER** push if uncertain about CI/CD success
- **NEVER** commit console.log/debug code
- **NEVER** commit credentials or secrets
- **NEVER** force push to main/master
- **NEVER** rewrite published history
- **NEVER** skip hooks (--no-verify)
- **NEVER** assume repository configuration

## SSH Configuration

### For Users with SSH Password

If your SSH key has password protection:

**Configuration in AGENTS.md or project settings:**

```yaml
git_workflow:
  auto_push: false
  push_mode: "manual"
  reason: "SSH key has password protection"
```

**AI Assistant Behavior:**
- ✅ Provide push commands in chat
- ✅ Wait for user manual execution
- ❌ Never attempt automatic push
- ❌ Never execute git push commands

### For Users with Passwordless SSH

```yaml
git_workflow:
  auto_push: true  # or prompt each time
  push_mode: "auto"
```

## Git Hooks

### Pre-commit Hook

Create `.git/hooks/pre-commit`:

```bash
#!/bin/sh

echo "Running pre-commit checks..."

# Run linter
npm run lint
if [ $? -ne 0 ]; then
  echo "❌ Linting failed. Commit aborted."
  exit 1
fi

# Run tests
npm test
if [ $? -ne 0 ]; then
  echo "❌ Tests failed. Commit aborted."
  exit 1
fi

# Run type check (if applicable)
if command -v tsc &> /dev/null; then
  npm run type-check
  if [ $? -ne 0 ]; then
    echo "❌ Type check failed. Commit aborted."
    exit 1
  fi
fi

echo "✅ All pre-commit checks passed!"
exit 0
```

### Pre-push Hook

Create `.git/hooks/pre-push`:

```bash
#!/bin/sh

echo "Running pre-push checks..."

# Run full test suite
npm test
if [ $? -ne 0 ]; then
  echo "❌ Tests failed. Push aborted."
  exit 1
fi

# Run build
npm run build
if [ $? -ne 0 ]; then
  echo "❌ Build failed. Push aborted."
  exit 1
fi

echo "✅ All pre-push checks passed!"
exit 0
```

Make hooks executable:
```bash
chmod +x .git/hooks/pre-commit
chmod +x .git/hooks/pre-push
```

## CI/CD Integration

### Before Providing Push Commands

**CRITICAL**: Only suggest push if confident about CI/CD success:

```
✅ Provide push command if:
- All local tests passed
- All linting passed
- Build succeeded
- Coverage meets threshold
- No warnings or errors
- Code follows AGENTS.md standards
- Similar changes passed CI/CD before

❌ DO NOT provide push command if:
- ANY quality check failed
- Uncertain about CI/CD requirements
- Making experimental changes
- First time working with this codebase
- User seems uncertain

Instead say:
"I recommend running the full CI/CD pipeline locally first to ensure 
the changes will pass. Once confirmed, you can push manually."
```

## GitHub MCP Server Integration

**If GitHub MCP Server is available**, use it for automated workflow monitoring.

### Workflow Validation After Push

```
After every git push (manual or auto):

1. Wait 5-10 seconds for workflows to trigger

2. Check workflow status via GitHub MCP:
   - List workflow runs for latest commit
   - Check status of each workflow

3. If workflows are RUNNING:
   ⏳ Report: "CI/CD workflows in progress..."
   ✅ Continue with other tasks
   ✅ Check again in next user interaction
   
4. If workflows COMPLETED:
   - All passed: ✅ Report success
   - Some failed: ❌ Fetch errors and fix

5. If workflows FAILED:
   a. Fetch complete error logs via GitHub MCP
   b. Display errors to user
   c. Analyze against AGENTS.md standards
   d. Propose specific fixes
   e. Implement fixes
   f. Run local quality checks
   g. Commit fixes
   h. Provide push command for retry
```

### Next Interaction Check

```
On every user message after a push:

if (github_mcp_available && last_push_timestamp) {
  // Check workflow status
  const status = await checkWorkflows();
  
  if (status.running) {
    console.log('⏳ CI/CD still running, will check later');
  } else if (status.failed) {
    console.log('❌ CI/CD failures detected!');
    await analyzeAndFixErrors(status.errors);
  } else {
    console.log('✅ All CI/CD workflows passed!');
  }
}
```

### Error Analysis Flow

```
When workflow fails:

1. Fetch error via GitHub MCP:
   - Workflow name
   - Job name  
   - Failed step
   - Error output
   - Full logs

2. Categorize error:
   - Test failure → Fix test or implementation
   - Lint error → Format/fix code style
   - Build error → Fix compilation issues
   - Type error → Fix type definitions
   - Coverage error → Add more tests

3. Fix following AGENTS.md:
   - Apply correct pattern from AGENTS.md
   - Add tests if needed
   - Verify locally before committing

4. Commit fix:
   git commit -m "fix: Resolve CI/CD failure - [specific issue]"

5. Provide push command:
   "Ready to retry. Run: git push origin main"

6. After next push:
   - Monitor again
   - Verify fix worked
```

### CI/CD Confidence Check

**Before suggesting push:**

```
Assess confidence in CI/CD success:

HIGH confidence (safe to push):
✅ All local checks passed
✅ Similar changes passed CI before
✅ No experimental changes
✅ Follows AGENTS.md exactly
✅ Comprehensive tests
✅ No unusual patterns

MEDIUM confidence (verify first):
⚠️ First time with this pattern
⚠️ Modified build configuration
⚠️ Changed dependencies
⚠️ Cross-platform concerns
→ Suggest: "Let's verify locally first"

LOW confidence (don't push yet):
❌ Experimental implementation
❌ Skipped some tests
❌ Uncertain about compatibility
❌ Modified CI/CD files
→ Say: "Let's run additional checks first"
```

## Troubleshooting

### Merge Conflicts

```bash
# View conflicts
git status

# Edit conflicted files (marked with <<<<<<<, =======, >>>>>>>)

# After resolving:
git add <resolved-files>
git commit -m "fix: Resolve merge conflicts"
```

### Accidental Commit

```bash
# Undo last commit, keep changes
git reset --soft HEAD~1

# Make corrections
# Re-commit properly
```

### Lost Commits

```bash
# View all actions
git reflog

# Recover lost commit
git checkout <commit-hash>
git checkout -b recovery-branch
```

<!-- GIT:END -->
