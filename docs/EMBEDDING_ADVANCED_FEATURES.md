# Advanced Embedding Features - Vectorizer

## Overview

Vectorizer implements a comprehensive embedding system supporting multiple state-of-the-art retrieval methods. This document details the advanced embedding capabilities, hybrid search pipelines, and evaluation frameworks implemented.

## üßÆ Embedding Methods

### Sparse Embeddings

#### TF-IDF (Term Frequency-Inverse Document Frequency)
- **Traditional baseline method** for text vectorization
- **Variable dimensionality** based on vocabulary size
- **Weighted term importance** using TF-IDF scoring
- **Efficient for exact keyword matching**

```rust
let tfidf = TfIdfEmbedding::new(vocabulary_size);
```

#### BM25 (Best Matching 25)
- **Advanced sparse retrieval** with probabilistic ranking
- **Configurable parameters**: k1=1.5, b=0.75 (standard values)
- **Document length normalization** to prevent bias toward longer documents
- **Superior relevance ranking** compared to TF-IDF

```rust
let bm25 = Bm25Embedding::new(vocabulary_size);
// Automatically uses k1=1.5, b=0.75
```

#### SVD-Reduced Embeddings (TF-IDF + SVD)
- **Dimensionality reduction** of TF-IDF vectors using SVD
- **Supported dimensions**: 300D and 768D (BERT-compatible)
- **Orthogonal transformation** preserving semantic relationships
- **Memory-efficient** dense representations

```rust
let svd = SvdEmbedding::new(300, vocabulary_size); // 300D reduction
svd.fit_svd(&documents)?; // Fit transformation matrix
```

### Dense Embeddings

#### BERT Embeddings (768D)
- **Contextual embeddings** capturing semantic meaning
- **768-dimensional vectors** (BERT-base compatible)
- **Placeholder implementation** ready for real BERT integration
- **Deterministic hashing** for reproducible results

```rust
let mut bert = BertEmbedding::new(768);
bert.load_model()?; // Placeholder for actual model loading
```

#### MiniLM Embeddings (384D)
- **Efficient sentence embeddings** optimized for speed
- **384-dimensional vectors** (MiniLM-L6-v2 compatible)
- **Fast inference** with good semantic quality
- **Placeholder implementation** ready for real model integration

```rust
let mut minilm = MiniLmEmbedding::new(384);
minilm.load_model()?; // Placeholder for actual model loading
```

## üîÑ Hybrid Search Pipeline

Vectorizer implements a sophisticated two-stage retrieval pipeline combining the best of sparse and dense methods:

### Stage 1: Sparse Retrieval (BM25/TF-IDF)
- **Efficient candidate selection** using sparse methods
- **Top-k retrieval** (default k=50) to limit computational cost
- **Fast ranking** based on term matching and document structure

### Stage 2: Dense Re-ranking
- **Semantic re-ranking** of sparse candidates using dense embeddings
- **Improved relevance** by considering contextual meaning
- **Configurable pipeline** supporting multiple combinations

### Supported Hybrid Combinations

#### BM25 + BERT Re-ranking
```rust
let hybrid = create_bm25_bert_hybrid(1000, 768, 50)?;
let results = hybrid.search_hybrid(query, &documents, 10)?;
```

#### BM25 + MiniLM Re-ranking
```rust
let hybrid = create_bm25_minilm_hybrid(1000, 384, 50)?;
let results = hybrid.search_hybrid(query, &documents, 10)?;
```

#### TF-IDF+SVD + BERT Re-ranking
```rust
let hybrid = create_tfidf_svd_bert_hybrid(1000, 300, 768, 50)?;
let results = hybrid.search_hybrid(query, &documents, 10)?;
```

## üìä Evaluation Framework

### Information Retrieval Metrics

#### Mean Reciprocal Rank (MRR)
- **Average reciprocal rank** of the first relevant document
- **Range**: [0, 1], higher is better
- **Formula**: MRR = (1/Q) √ó Œ£(1/rank_i)

#### Mean Average Precision (MAP)
- **Average precision** across all relevant documents
- **Range**: [0, 1], higher is better
- **Formula**: MAP = (1/Q) √ó Œ£(AP_i)

#### Precision@K
- **Fraction of relevant documents** in top-K results
- **Formula**: P@K = (relevant_in_top_k) / K

#### Recall@K
- **Fraction of relevant documents** retrieved in top-K
- **Formula**: R@K = (relevant_retrieved) / total_relevant

### Benchmark Suite

The comprehensive benchmark evaluates 8 embedding methods:

| Method | Type | Dimensions | Use Case |
|--------|------|------------|----------|
| TF-IDF | Sparse | Variable | Baseline, exact matching |
| BM25 | Sparse | Variable | Relevance ranking, efficiency |
| TF-IDF+SVD(300D) | Reduced | 300D | Memory-efficient dense |
| TF-IDF+SVD(768D) | Reduced | 768D | BERT-compatible dense |
| BERT | Dense | 768D | Semantic understanding |
| MiniLM | Dense | 384D | Fast semantic search |
| BM25+BERT | Hybrid | 768D | Best relevance + efficiency |
| BM25+MiniLM | Hybrid | 384D | Balanced performance |

### Running Benchmarks

```bash
# Run comprehensive embedding comparison
cargo run --example benchmark_embeddings

# Results saved to benchmark_report_TIMESTAMP.md
```

## üèóÔ∏è Architecture

### Modular Embedding System

```rust
pub trait EmbeddingProvider: Send + Sync {
    fn embed_batch(&self, texts: &[&str]) -> Result<Vec<Vec<f32>>>;
    fn embed(&self, text: &str) -> Result<Vec<f32>>;
    fn dimension(&self) -> usize;
}
```

### Embedding Manager

```rust
let mut manager = EmbeddingManager::new();
manager.register_provider("bm25".to_string(), Box::new(bm25))?;
manager.set_default_provider("bm25")?;
```

### Hybrid Retriever

```rust
pub struct HybridRetriever<T: EmbeddingProvider, U: EmbeddingProvider> {
    sparse_retriever: T,
    dense_reranker: U,
    first_stage_k: usize,
}
```

## üìà Performance Characteristics

### Sparse Methods (TF-IDF, BM25)
- **‚úÖ Fast indexing** and retrieval
- **‚úÖ Memory efficient** for large vocabularies
- **‚úÖ Exact term matching** capabilities
- **‚ùå Limited semantic understanding**

### Dense Methods (BERT, MiniLM)
- **‚úÖ Semantic understanding** and context
- **‚úÖ Robust to paraphrasing** and synonyms
- **‚úÖ Transfer learning** capabilities
- **‚ùå Higher computational cost**

### Hybrid Methods
- **‚úÖ Combines best of both worlds**
- **‚úÖ Efficient candidate selection + accurate ranking**
- **‚úÖ Scalable** with configurable first-stage k
- **‚úÖ Production-ready** performance characteristics

## üîß Technical Implementation

### SVD Implementation
- **Gram-Schmidt orthogonalization** for stable basis
- **Seeded random initialization** for reproducibility
- **Memory-efficient** matrix operations
- **No external LAPACK/BLAS** dependencies

### Evaluation Framework
- **Comprehensive metrics** calculation
- **Batch processing** for efficiency
- **Ground truth support** for supervised evaluation
- **Markdown reporting** for documentation

### REST API Integration
```bash
# Text search with automatic embeddings
curl -X POST http://localhost:15001/api/v1/collections/docs/search/text \
  -H "Content-Type: application/json" \
  -d '{"query": "machine learning", "limit": 5}'
```

## üöÄ Future Extensions

### Real Model Integration
- **Hugging Face Transformers** integration
- **ONNX Runtime** for optimized inference
- **GPU acceleration** support
- **Model quantization** for efficiency

### Advanced Hybrid Methods
- **Query routing** based on query type
- **Ensemble methods** combining multiple rankings
- **Learning-to-rank** for optimized combinations
- **Adaptive k selection** based on query characteristics

### Production Optimizations
- **Embedding caching** for repeated queries
- **Approximate nearest neighbor** for dense retrieval
- **Distributed indexing** for large-scale deployment
- **Real-time embedding updates**

## üìö References

- **BM25**: Robertson, S., & Zaragoza, H. (2009). The probabilistic relevance framework
- **SVD**: Golub, G. H., & Van Loan, C. F. (2013). Matrix computations
- **Hybrid Search**: Xiong, L., et al. (2021). Approximate nearest neighbor negative contrastive learning
- **Evaluation Metrics**: Manning, C. D., et al. (2008). Introduction to information retrieval

---

*Advanced embedding features implemented in Vectorizer v0.4.0*
